<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>3.4.&nbsp;The Important Configurations</title><link rel="stylesheet" href="css/freebsd_docbook.css" type="text/css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.75.2"><link rel="home" href="book.html" title="The Apache HBase Book"><link rel="up" href="configuration.html" title="Chapter&nbsp;3.&nbsp;Configuration"><link rel="prev" href="log4j.html" title="3.3.&nbsp;log4j.properties"><link rel="next" href="client_dependencies.html" title="3.5.&nbsp;Client configuration and dependencies connecting to an HBase cluster"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">3.4.&nbsp;The Important Configurations</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="log4j.html">Prev</a>&nbsp;</td><th width="60%" align="center">Chapter&nbsp;3.&nbsp;Configuration</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="client_dependencies.html">Next</a></td></tr></table><hr></div><div class="section" title="3.4.&nbsp;The Important Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="important_configurations"></a>3.4.&nbsp;The Important Configurations</h2></div></div></div><p>Below we list what the <span class="emphasis"><em>important</em></span>
      Configurations.  We've divided this section into
      required configuration and worth-a-look recommended configs.
      </p><div class="section" title="3.4.1.&nbsp;Required Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="required_configuration"></a>3.4.1.&nbsp;Required Configurations</h3></div></div></div><p>See <a class="xref" href="notsoquick.html#requirements" title="1.3.1.&nbsp;Requirements">Section&nbsp;1.3.1, &#8220;Requirements&#8221;</a>.
      It lists at least two required configurations needed running HBase bearing
      load: i.e. <a class="xref" href="notsoquick.html#ulimit" title="1.3.1.6.&nbsp; ulimit and nproc">Section&nbsp;1.3.1.6, &#8220;
          <code class="varname">ulimit</code>
            and
          <code class="varname">nproc</code>
        &#8221;</a> and
      <a class="xref" href="notsoquick.html#dfs.datanode.max.xcievers" title="1.3.1.7.&nbsp;dfs.datanode.max.xcievers">Section&nbsp;1.3.1.7, &#8220;<code class="varname">dfs.datanode.max.xcievers</code>&#8221;</a>.
      </p></div><div class="section" title="3.4.2.&nbsp;Recommended Configuations"><div class="titlepage"><div><div><h3 class="title"><a name="recommended_configurations"></a>3.4.2.&nbsp;Recommended Configuations</h3></div></div></div><div class="section" title="3.4.2.1.&nbsp;zookeeper.session.timeout"><div class="titlepage"><div><div><h4 class="title"><a name="zookeeper.session.timeout"></a>3.4.2.1.&nbsp;<code class="varname">zookeeper.session.timeout</code></h4></div></div></div><p>The default timeout is three minutes (specified in milliseconds). This means
              that if a server crashes, it will be three minutes before the Master notices
              the crash and starts recovery. You might like to tune the timeout down to
              a minute or even less so the Master notices failures the sooner.
              Before changing this value, be sure you have your JVM garbage collection
              configuration under control otherwise, a long garbage collection that lasts
              beyond the ZooKeeper session timeout will take out
              your RegionServer (You might be fine with this -- you probably want recovery to start
          on the server if a RegionServer has been in GC for a long period of time).</p><p>To change this configuration, edit <code class="filename">hbase-site.xml</code>,
          copy the changed file around the cluster and restart.</p><p>We set this value high to save our having to field noob questions up on the mailing lists asking
              why a RegionServer went down during a massive import.  The usual cause is that their JVM is untuned and
              they are running into long GC pauses.  Our thinking is that
              while users are  getting familiar with HBase, we'd save them having to know all of its
              intricacies.  Later when they've built some confidence, then they can play
              with configuration such as this.
          </p></div><div class="section" title="3.4.2.2.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.handler.count"></a>3.4.2.2.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h4></div></div></div><p>
          This setting defines the number of threads that are kept open to answer
          incoming requests to user tables. The default of 10 is rather low in order to
          prevent users from killing their region servers when using large write buffers
          with a high number of concurrent clients. The rule of thumb is to keep this
          number low when the payload per request approaches the MB (big puts, scans using
          a large cache) and high when the payload is small (gets, small puts, ICVs, deletes).
          </p><p>
          It is safe to set that number to the
          maximum number of incoming clients if their payload is small, the typical example
          being a cluster that serves a website since puts aren't typically buffered
          and most of the operations are gets.
          </p><p>
          The reason why it is dangerous to keep this setting high is that the aggregate
          size of all the puts that are currently happening in a region server may impose
          too much pressure on its memory, or even trigger an OutOfMemoryError. A region server
          running on low memory will trigger its JVM's garbage collector to run more frequently
          up to a point where GC pauses become noticeable (the reason being that all the memory
          used to keep all the requests' payloads cannot be trashed, no matter how hard the
          garbage collector tries). After some time, the overall cluster
          throughput is affected since every request that hits that region server will take longer,
          which exacerbates the problem even more.
          </p></div><div class="section" title="3.4.2.3.&nbsp;Configuration for large memory machines"><div class="titlepage"><div><div><h4 class="title"><a name="big_memory"></a>3.4.2.3.&nbsp;Configuration for large memory machines</h4></div></div></div><p>
          HBase ships with a reasonable, conservative configuration that will
          work on nearly all
          machine types that people might want to test with. If you have larger
          machines -- HBase has 8G and larger heap -- you might the following configuration options helpful.
          TODO.
        </p></div><div class="section" title="3.4.2.4.&nbsp;LZO compression"><div class="titlepage"><div><div><h4 class="title"><a name="lzo"></a>3.4.2.4.&nbsp;LZO compression<a class="indexterm" name="d0e1959"></a></h4></div></div></div><p>You should consider enabling LZO compression.  Its
      near-frictionless and in most all cases boosts performance.
      </p><p>Unfortunately, HBase cannot ship with LZO because of
      the licensing issues; HBase is Apache-licensed, LZO is GPL.
      Therefore LZO install is to be done post-HBase install.
      See the <a class="link" href="http://wiki.apache.org/hadoop/UsingLzoCompression" target="_top">Using LZO Compression</a>
      wiki page for how to make LZO work with HBase.
      </p><p>A common problem users run into when using LZO is that while initial
      setup of the cluster runs smooth, a month goes by and some sysadmin goes to
      add a machine to the cluster only they'll have forgotten to do the LZO
      fixup on the new machine.  In versions since HBase 0.90.0, we should
      fail in a way that makes it plain what the problem is, but maybe not.
      Remember you read this paragraph<sup>[<a name="d0e1971" href="#ftn.d0e1971" class="footnote">11</a>]</sup>.
      </p><p>See also <a class="xref" href="compression.html" title="Appendix&nbsp;B.&nbsp;Compression In HBase">Appendix&nbsp;B, <i>Compression In HBase</i></a>
      at the tail of this book.</p></div><div class="section" title="3.4.2.5.&nbsp;Bigger Regions"><div class="titlepage"><div><div><h4 class="title"><a name="bigger.regions"></a>3.4.2.5.&nbsp;Bigger Regions</h4></div></div></div><p>
      Consider going to larger regions to cut down on the total number of regions
      on your cluster. Generally less Regions to manage makes for a smoother running
      cluster (You can always later manually split the big Regions should one prove
      hot and you want to spread the request load over the cluster).  By default,
      regions are 256MB in size.  You could run with
      1G.  Some run with even larger regions; 4G or even larger.  Adjust
      <code class="code">hbase.hregion.max.filesize</code> in your <code class="filename">hbase-site.xml</code>.
      </p></div><div class="section" title="3.4.2.6.&nbsp;Managed Splitting"><div class="titlepage"><div><div><h4 class="title"><a name="disable.splitting"></a>3.4.2.6.&nbsp;Managed Splitting</h4></div></div></div><p>
      Rather than let HBase auto-split your Regions, manage the splitting manually
      <sup>[<a name="d0e1997" href="#ftn.d0e1997" class="footnote">12</a>]</sup>.
 With growing amounts of data, splits will continually be needed. Since
 you always know exactly what regions you have, long-term debugging and
 profiling is much easier with manual splits. It is hard to trace the logs to
 understand region level problems if it keeps splitting and getting renamed.
 Data offlining bugs + unknown number of split regions == oh crap! If an
 <code class="classname">HLog</code> or <code class="classname">StoreFile</code>
 was mistakenly unprocessed by HBase due to a weird bug and
 you notice it a day or so later, you can be assured that the regions
 specified in these files are the same as the current regions and you have
 less headaches trying to restore/replay your data.
 You can finely tune your compaction algorithm. With roughly uniform data
 growth, it's easy to cause split / compaction storms as the regions all
 roughly hit the same data size at the same time. With manual splits, you can
 let staggered, time-based major compactions spread out your network IO load.
      </p><p>
 How do I turn off automatic splitting? Automatic splitting is determined by the configuration value
 <code class="code">hbase.hregion.max.filesize</code>. It is not recommended that you set this
 to <code class="varname">Long.MAX_VALUE</code> in case you forget about manual splits. A suggested setting
 is 100GB, which would result in &gt; 1hr major compactions if reached.
 </p><p>What's the optimal number of pre-split regions to create?
 Mileage will vary depending upon your application.
 You could start low with 10 pre-split regions / server and watch as data grows
 over time. It's better to err on the side of too little regions and rolling split later.
 A more complicated answer is that this depends upon the largest storefile
 in your region. With a growing data size, this will get larger over time. You
 want the largest region to be just big enough that the <code class="classname">Store</code> compact
 selection algorithm only compacts it due to a timed major. If you don't, your
 cluster can be prone to compaction storms as the algorithm decides to run
 major compactions on a large series of regions all at once. Note that
 compaction storms are due to the uniform data growth, not the manual split
 decision.
 </p><p> If you pre-split your regions too thin, you can increase the major compaction
interval by configuring <code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code>. If your data size
grows too large, use the (post-0.90.0 HBase) <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code>
script to perform a network IO safe rolling split
of all regions.
</p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a name="ftn.d0e1971" href="#d0e1971" class="para">11</a>] </sup>See
              <a class="xref" href="hbase.regionserver.codecs.html" title="B.2.&nbsp; hbase.regionserver.codecs">Section&nbsp;B.2, &#8220;
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    &#8221;</a>
      for a feature to help protect against failed LZO install</p></div><div class="footnote"><p><sup>[<a name="ftn.d0e1997" href="#d0e1997" class="para">12</a>] </sup>What follows is taken from the javadoc at the head of
      the <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code> tool
      added to HBase post-0.90.0 release.
      </p></div></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="log4j.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="configuration.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="client_dependencies.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">3.3.&nbsp;<code class="filename">log4j.properties</code>&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;3.5.&nbsp;Client configuration and dependencies connecting to an HBase cluster</td></tr></table></div></body></html>