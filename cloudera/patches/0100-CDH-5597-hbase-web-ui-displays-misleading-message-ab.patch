From 80d2619fa6b7e355dfe3a66a5b566b92fff27c5b Mon Sep 17 00:00:00 2001
From: David S. Wang <dsw@cloudera.com>
Date: Wed, 2 May 2012 13:12:27 -0700
Subject: [PATCH 100/101] CDH-5597 hbase web ui displays misleading message about hdfs append
 Change append checks to sync and hflush checks.

Reason: Bug
Author: David S. Wang
Ref: CDH-5597
---
 .../hbase/tmpl/master/MasterStatusTmpl.jamon       |    7 +-
 .../hadoop/hbase/master/MasterStatusServlet.java   |   18 +++++-
 .../hbase/regionserver/wal/HLogSplitter.java       |    2 +-
 .../java/org/apache/hadoop/hbase/util/FSUtils.java |   60 ++++++++++---------
 .../apache/hadoop/hbase/util/RegionSplitter.java   |    2 +-
 .../hadoop/hbase/regionserver/wal/TestHLog.java    |    2 +-
 .../hbase/regionserver/wal/TestLogRollAbort.java   |    6 +-
 .../hbase/regionserver/wal/TestLogRolling.java     |   12 ++--
 .../org/apache/hadoop/hbase/util/TestFSUtils.java  |    5 +-
 9 files changed, 63 insertions(+), 51 deletions(-)

diff --git a/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon b/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon
index 86d4e65..9187aab 100644
--- a/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon
+++ b/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon
@@ -25,7 +25,8 @@ HServerAddress rootLocation = null;
 HServerAddress  metaLocation = null;
 Map<String, HServerInfo> servers = null;
 Set<String> deadServers = null;
-boolean showAppendWarning = false;
+boolean showSyncWarning = false;
+boolean showHflushWarning = false;
 int interval = 1;
 </%args>
 <%import>
@@ -70,9 +71,9 @@ org.apache.hadoop.hbase.HTableDescriptor;
   for details.
   </div>
 </%if>
-<%if showAppendWarning %>
+<%if showSyncWarning && showHflushWarning %>
   <div class="warning">
-  You are currently running the HMaster without HDFS append support enabled.
+  You are currently running the HMaster without either HDFS sync or hflush support enabled.
   This may result in data loss.
   Please see the <a href="http://wiki.apache.org/hadoop/Hbase/HdfsSyncSupport">HBase wiki</a>
   for details.
diff --git a/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java b/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
index 3060e3f..17de358 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
@@ -70,7 +70,8 @@ public class MasterStatusServlet extends HttpServlet {
     response.setContentType("text/html");
     new MasterStatusTmpl()
       .setFrags(frags)
-      .setShowAppendWarning(shouldShowAppendWarning(conf))
+      .setShowSyncWarning(shouldShowSyncWarning(conf))
+      .setShowHflushWarning(shouldShowHflushWarning(conf))
       .setRootLocation(rootLocation)
       .setMetaLocation(metaLocation)
       .setServers(servers)
@@ -100,11 +101,20 @@ public class MasterStatusServlet extends HttpServlet {
     }
   }
 
-  static boolean shouldShowAppendWarning(Configuration conf) {
+  static boolean shouldShowSyncWarning(Configuration conf) {
     try {
-      return !FSUtils.isAppendSupported(conf) && FSUtils.isHDFS(conf);
+      return !FSUtils.isSyncSupported() && FSUtils.isHDFS(conf);
     } catch (IOException e) {
-      LOG.warn("Unable to determine if append is supported", e);
+      LOG.warn("Unable to determine if sync is supported", e);
+      return false;
+    }
+  }
+
+  static boolean shouldShowHflushWarning(Configuration conf) {
+    try {
+      return !FSUtils.isHflushSupported() && FSUtils.isHDFS(conf);
+    } catch (IOException e) {
+      LOG.warn("Unable to determine if hflush is supported", e);
       return false;
     }
   }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
index 83f57d6..993320b 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
@@ -745,7 +745,7 @@ public class HLogSplitter {
     }
 
     try {
-      recoverFileLease(fs, path, conf);
+      recoverFileLease(fs, path);
       try {
         in = getReader(fs, path, conf);
       } catch (EOFException e) {
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
index 8ebdecf..1da2143 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
@@ -619,33 +619,37 @@ public class FSUtils {
   }
 
   /**
-   * Heuristic to determine whether is safe or not to open a file for append
-   * Looks both for dfs.support.append and use reflection to search
-   * for SequenceFile.Writer.syncFs() or FSDataOutputStream.hflush()
-   * @param conf
-   * @return True if append support
+   * Heuristic to determine whether is safe or not to open a file for sync
+   * Uses reflection to search for SequenceFile.Writer.syncFs()
+   * @return True if sync is supported
    */
-  public static boolean isAppendSupported(final Configuration conf) {
-    boolean append = conf.getBoolean("dfs.support.append", false);
-    if (append) {
-      try {
-        // TODO: The implementation that comes back when we do a createWriter
-        // may not be using SequenceFile so the below is not a definitive test.
-        // Will do for now (hdfs-200).
-        SequenceFile.Writer.class.getMethod("syncFs", new Class<?> []{});
-        append = true;
-      } catch (SecurityException e) {
-      } catch (NoSuchMethodException e) {
-        append = false;
-      }
-    } else {
-      try {
-        FSDataOutputStream.class.getMethod("hflush", new Class<?> []{});
-      } catch (NoSuchMethodException e) {
-        append = false;
-      }
+  public static boolean isSyncSupported() {
+    boolean sync = true;
+    try {
+      // TODO: The implementation that comes back when we do a createWriter
+      // may not be using SequenceFile so the below is not a definitive test.
+      // Will do for now (hdfs-200).
+      SequenceFile.Writer.class.getMethod("syncFs", new Class<?> []{});
+    } catch (SecurityException e) {
+    } catch (NoSuchMethodException e) {
+      sync = false;
+    }
+    return sync;
+  }
+
+  /**
+   * Heuristic to determine whether is safe or not to open a file for hflush
+   * Uses reflection to search for FSDataOutputStream.hflush()
+   * @return True if hflush is supported
+   */
+  public static boolean isHflushSupported() {
+    boolean hflush = true;
+    try {
+      FSDataOutputStream.class.getMethod("hflush", new Class<?> []{});
+    } catch (NoSuchMethodException e) {
+      hflush = false;
     }
-    return append;
+    return hflush;
   }
 
   /**
@@ -666,10 +670,10 @@ public class FSUtils {
    * @param append True if append supported
    * @throws IOException
    */
-  public static void recoverFileLease(final FileSystem fs, final Path p, Configuration conf)
+  public static void recoverFileLease(final FileSystem fs, final Path p)
   throws IOException{
-    if (!isAppendSupported(conf)) {
-      LOG.warn("Running on HDFS without append enabled may result in data loss");
+    if (!isSyncSupported() && !isHflushSupported()) {
+      LOG.warn("Running on HDFS without sync or hflush enabled may result in data loss");
       return;
     }
     // lease recovery not needed for local file system case.
diff --git a/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java b/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
index e42b049..708e77e 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
@@ -672,7 +672,7 @@ public class RegionSplitter {
       fs.rename(tmpFile, splitFile);
     } else {
       LOG.debug("_balancedSplit file found. Replay log to restore state...");
-      FSUtils.recoverFileLease(fs, splitFile, table.getConfiguration());
+      FSUtils.recoverFileLease(fs, splitFile);
 
       // parse split file and process remaining splits
       FSDataInputStream tmpIn = fs.open(splitFile);
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
index 760494b..d403356 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
@@ -399,7 +399,7 @@ public class TestHLog  {
       public Exception exception = null;
       public void run() {
           try {
-            FSUtils.recoverFileLease(recoveredFs, walPath, rlConf);
+            FSUtils.recoverFileLease(recoveredFs, walPath);
           } catch (IOException e) {
             exception = e;
           }
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
index 200bacc..ff835cb 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
@@ -141,9 +141,9 @@ public class TestLogRollAbort {
     HLog log = server.getWAL();
 
     assertTrue("Need HDFS-826 for this test", log.canGetCurReplicas());
-    // don't run this test without append support (HDFS-200 & HDFS-142)
-    assertTrue("Need append support for this test",
-        FSUtils.isAppendSupported(TEST_UTIL.getConfiguration()));
+    // don't run this test without sync or hflush support
+    assertTrue("Need sync or hflush support for this test",
+        FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
 
     Put p = new Put(Bytes.toBytes("row2001"));
     p.add(HConstants.CATALOG_FAMILY, Bytes.toBytes("col"), Bytes.toBytes(2001));
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
index e063add..ab84ab8 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
@@ -332,9 +332,9 @@ public class TestLogRolling  {
     this.log = server.getWAL();
 
     assertTrue("Need HDFS-826 for this test", log.canGetCurReplicas());
-    // don't run this test without append support (HDFS-200 & HDFS-142)
-    assertTrue("Need append support for this test", FSUtils
-        .isAppendSupported(TEST_UTIL.getConfiguration()));
+    // don't run this test without sync or hflush support 
+    assertTrue("Need sync or hflush support for this test",
+        FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
 
     // add up the datanode count, to ensure proper replication when we kill 1
     dfsCluster
@@ -436,9 +436,9 @@ public class TestLogRolling  {
     });
 
     assertTrue("Need HDFS-826 for this test", log.canGetCurReplicas());
-    // don't run this test without append support (HDFS-200 & HDFS-142)
-    assertTrue("Need append support for this test", FSUtils
-        .isAppendSupported(TEST_UTIL.getConfiguration()));
+    // don't run this test without sync or hflush support 
+    assertTrue("Need sync or hflush support for this test",
+        FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
 
     writeData(table, 1002);
 
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java b/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
index c8fc065..f0b0039 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
@@ -32,15 +32,12 @@ import org.junit.Test;
 public class TestFSUtils {
   @Test public void testIsHDFS() throws Exception {
     HBaseTestingUtility htu = new HBaseTestingUtility();
-    htu.getConfiguration().setBoolean("dfs.support.append", false);
     assertFalse(FSUtils.isHDFS(htu.getConfiguration()));
-    assertFalse(FSUtils.isAppendSupported(htu.getConfiguration()));
-    htu.getConfiguration().setBoolean("dfs.support.append", true);
+    assertTrue(FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
     MiniDFSCluster cluster = null;
     try {
       cluster = htu.startMiniDFSCluster(1);
       assertTrue(FSUtils.isHDFS(htu.getConfiguration()));
-      assertTrue(FSUtils.isAppendSupported(htu.getConfiguration()));
     } finally {
       if (cluster != null) cluster.shutdown();
     }
-- 
1.7.0.4

