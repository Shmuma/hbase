From 83e7e3edb3e3b9963c3611e795c4e6506ee20eec Mon Sep 17 00:00:00 2001
From: Michael Stack <stack@apache.org>
Date: Thu, 19 May 2011 23:00:47 +0000
Subject: [PATCH 07/18] HBASE-3691 Add compressor support for 'snappy', google's compressor

git-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1125163 13f79535-47bb-0310-9956-ffa450edef68
---
 src/docbkx/book.xml                                |   39 ++++
 .../org/apache/hadoop/hbase/HColumnDescriptor.java |    2 +
 .../apache/hadoop/hbase/io/hfile/Compression.java  |   19 ++
 .../apache/hadoop/hbase/util/CompressionTest.java  |    3 +-
 .../hbase/io/hfile/TestHFilePerformance.java       |    2 +-
 .../hadoop/hbase/io/hfile/TestHFileSeek.java       |    4 +-
 .../hbase/mapreduce/TestHFileOutputFormat.java     |  202 ++++++++++++++++++++
 .../hadoop/hbase/util/TestCompressionTest.java     |    1 +
 8 files changed, 268 insertions(+), 4 deletions(-)

diff --git a/src/docbkx/book.xml b/src/docbkx/book.xml
index ac33d88..a7894c0 100644
--- a/src/docbkx/book.xml
+++ b/src/docbkx/book.xml
@@ -1311,6 +1311,45 @@ false
     reports in your logs; see <xref linkend="brand.new.compressor" />).
     </para>
     </section>
+    <section xml:id="snappy.compression">
+    <title>
+    SNAPPY
+    </title>
+    <para>
+        To set snappy compression on a column family, do as following:
+        <orderedlist>
+            <listitem>
+                <para>
+                 Install hadoop-snappy using these instructions: http://code.google.com/p/hadoop-snappy/
+                </para>
+            </listitem>
+            <listitem>
+                <para>
+                    You need to ensure the hadoop-snappy libs (incl. the native libs) are in the HBase classpath. One way to do this is
+                    to just symlink the libs from <filename>HADOOP_HOME/lib</filename> to <filename>HBASE_HOME/lib</filename>.
+                    This needs to be done on all HBase nodes, as with LZO.
+                </para>
+            </listitem>
+            <listitem>
+                <para>
+        Use CompressionTest to verify snappy support is enabled and the libs can be loaded:
+        <programlisting>$ hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://host/path/to/hbase snappy</programlisting>
+                </para>
+            </listitem>
+            <listitem>
+                <para>
+        Create a column family with snappy compression and verify it in the hbase shell:
+        <programlisting>$ hbase> create 't1', { NAME => 'cf1', COMPRESSION => 'snappy' }
+hbase> describe 't1'</programlisting>
+        In the output of the "describe" command, you need to ensure it lists "COMPRESSION => 'snappy'"
+                </para>
+            </listitem>
+
+        </orderedlist>
+
+    </para>
+    </section>
+
   </appendix>
 
   <appendix xml:id="faq">
diff --git a/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java b/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
index 8808c06..76f346e 100644
--- a/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
+++ b/src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
@@ -432,6 +432,7 @@ public class HColumnDescriptor implements WritableComparable<HColumnDescriptor>
     switch (type) {
       case LZO: compressionType = "LZO"; break;
       case GZ: compressionType = "GZ"; break;
+      case SNAPPY: compressionType = "SNAPPY"; break;
       default: compressionType = "NONE"; break;
     }
     setValue(COMPRESSION, compressionType);
@@ -456,6 +457,7 @@ public class HColumnDescriptor implements WritableComparable<HColumnDescriptor>
     switch (type) {
       case LZO: compressionType = "LZO"; break;
       case GZ: compressionType = "GZ"; break;
+      case SNAPPY: compressionType = "SNAPPY"; break;
       default: compressionType = "NONE"; break;
     }
     setValue(COMPRESSION_COMPACT, compressionType);
diff --git a/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java b/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java
index 0b7f293..0bfc7e2 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java
@@ -140,6 +140,25 @@ public final class Compression {
 
         return downStream;
       }
+    },
+    SNAPPY("snappy") {
+        // Use base type to avoid compile-time dependencies.
+        private transient CompressionCodec snappyCodec;
+
+        @Override
+        CompressionCodec getCodec(Configuration conf) {
+          if (snappyCodec == null) {
+            try {
+              Class<?> externalCodec =
+                  ClassLoader.getSystemClassLoader().loadClass("org.apache.hadoop.io.compress.SnappyCodec");
+              snappyCodec = (CompressionCodec) ReflectionUtils.newInstance(externalCodec, 
+                  conf);
+            } catch (ClassNotFoundException e) {
+              throw new RuntimeException(e);
+            }
+          }
+          return snappyCodec;
+        }
     };
 
     private final Configuration conf;
diff --git a/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java b/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java
index f255e50..4e59f50 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java
@@ -94,10 +94,11 @@ public class CompressionTest {
 
   public static void usage() {
     System.err.println(
-      "Usage: CompressionTest <path> none|gz|lzo\n" +
+      "Usage: CompressionTest <path> none|gz|lzo|snappy\n" +
       "\n" +
       "For example:\n" +
       "  hbase " + CompressionTest.class + " file:///tmp/testfile gz\n");
+    System.exit(1);
   }
 
   public static void doSmokeTest(FileSystem fs, Path path, String codec)
diff --git a/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java b/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java
index d99fc1c..f2e7e9f 100644
--- a/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java
+++ b/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java
@@ -129,7 +129,7 @@ public class TestHFilePerformance extends TestCase {
    * @param fileType "HFile" or "SequenceFile"
    * @param keyLength
    * @param valueLength
-   * @param codecName "none", "lzo", "gz"
+   * @param codecName "none", "lzo", "gz", "snappy"
    * @param rows number of rows to be written.
    * @param writeMethod used for HFile only.
    * @param minBlockSize used for HFile only.
diff --git a/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java b/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java
index 307e642..b7c5911 100644
--- a/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java
+++ b/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java
@@ -281,7 +281,7 @@ public class TestHFileSeek extends TestCase {
 
     private Options buildOptions() {
       Option compress =
-          OptionBuilder.withLongOpt("compress").withArgName("[none|lzo|gz]")
+          OptionBuilder.withLongOpt("compress").withArgName("[none|lzo|gz|snappy]")
               .hasArg().withDescription("compression scheme").create('c');
 
       Option fileSize =
@@ -446,7 +446,7 @@ public class TestHFileSeek extends TestCase {
 
     private void validateOptions() throws ParseException {
       if (!compress.equals("none") && !compress.equals("lzo")
-          && !compress.equals("gz")) {
+          && !compress.equals("gz") && !compress.equals("snappy")) {
         throw new ParseException("Unknown compression scheme: " + compress);
       }
 
diff --git a/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java b/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
index ecf7223..c77e877 100644
--- a/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
+++ b/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
@@ -372,6 +372,208 @@ public class TestHFileOutputFormat  {
     assertTrue(job.waitForCompletion(true));
   }
   
+  /**
+   * Test for
+   * {@link HFileOutputFormat#createFamilyCompressionMap(Configuration)}. Tests
+   * that the compression map is correctly deserialized from configuration
+   * 
+   * @throws IOException
+   */
+  @Test
+  public void testCreateFamilyCompressionMap() throws IOException {
+    for (int numCfs = 0; numCfs <= 3; numCfs++) {
+      Configuration conf = new Configuration(this.util.getConfiguration());
+      Map<String, Compression.Algorithm> familyToCompression = getMockColumnFamilies(numCfs);
+      HTable table = Mockito.mock(HTable.class);
+      setupMockColumnFamilies(table, familyToCompression);
+      HFileOutputFormat.configureCompression(table, conf);
+
+      // read back family specific compression setting from the configuration
+      Map<byte[], String> retrievedFamilyToCompressionMap = HFileOutputFormat.createFamilyCompressionMap(conf);
+
+      // test that we have a value for all column families that matches with the
+      // used mock values
+      for (Entry<String, Algorithm> entry : familyToCompression.entrySet()) {
+        assertEquals("Compression configuration incorrect for column family:" + entry.getKey(), entry.getValue()
+                     .getName(), retrievedFamilyToCompressionMap.get(entry.getKey().getBytes()));
+      }
+    }
+  }
+
+  private void setupMockColumnFamilies(HTable table,
+    Map<String, Compression.Algorithm> familyToCompression) throws IOException
+  {
+    HTableDescriptor mockTableDescriptor = new HTableDescriptor(TABLE_NAME);
+    for (Entry<String, Compression.Algorithm> entry : familyToCompression.entrySet()) {
+      mockTableDescriptor.addFamily(new HColumnDescriptor(entry.getKey().getBytes(), 1, entry.getValue().getName(),
+                                                          false, false, 0, "none"));
+    }
+    Mockito.doReturn(mockTableDescriptor).when(table).getTableDescriptor();
+  }
+
+  private void setupMockStartKeys(HTable table) throws IOException {
+    byte[][] mockKeys = new byte[][] {
+        HConstants.EMPTY_BYTE_ARRAY,
+        Bytes.toBytes("aaa"),
+        Bytes.toBytes("ggg"),
+        Bytes.toBytes("zzz")
+    };
+    Mockito.doReturn(mockKeys).when(table).getStartKeys();
+  }
+
+  /**
+   * @return a map from column family names to compression algorithms for
+   *         testing column family compression. Column family names have special characters
+   */
+  private Map<String, Compression.Algorithm> getMockColumnFamilies(int numCfs) {
+    Map<String, Compression.Algorithm> familyToCompression = new HashMap<String, Compression.Algorithm>();
+    // use column family names having special characters
+    if (numCfs-- > 0) {
+      familyToCompression.put("Family1!@#!@#&", Compression.Algorithm.LZO);
+    }
+    if (numCfs-- > 0) {
+      familyToCompression.put("Family2=asdads&!AASD", Compression.Algorithm.SNAPPY);
+    }
+    if (numCfs-- > 0) {
+      familyToCompression.put("Family2=asdads&!AASD", Compression.Algorithm.GZ);
+    }
+    if (numCfs-- > 0) {
+      familyToCompression.put("Family3", Compression.Algorithm.NONE);
+    }
+    return familyToCompression;
+  }
+  
+  /**
+   * Test that {@link HFileOutputFormat} RecordWriter uses compression settings
+   * from the column family descriptor
+   */
+  @Test
+  public void testColumnFamilyCompression()
+      throws IOException, InterruptedException {
+    Configuration conf = new Configuration(this.util.getConfiguration());
+    RecordWriter<ImmutableBytesWritable, KeyValue> writer = null;
+    TaskAttemptContext context = null;
+    Path dir =
+        HBaseTestingUtility.getTestDir("testColumnFamilyCompression");
+
+    HTable table = Mockito.mock(HTable.class);
+
+    Map<String, Compression.Algorithm> configuredCompression =
+      new HashMap<String, Compression.Algorithm>();
+    Compression.Algorithm[] supportedAlgos = getSupportedCompressionAlgorithms();
+
+    int familyIndex = 0;
+    for (byte[] family : FAMILIES) {
+      configuredCompression.put(Bytes.toString(family),
+                                supportedAlgos[familyIndex++ % supportedAlgos.length]);
+    }
+    setupMockColumnFamilies(table, configuredCompression);
+
+    // set up the table to return some mock keys
+    setupMockStartKeys(table);
+
+    try {
+      // partial map red setup to get an operational writer for testing
+      Job job = new Job(conf, "testLocalMRIncrementalLoad");
+      setupRandomGeneratorMapper(job);
+      HFileOutputFormat.configureIncrementalLoad(job, table);
+      FileOutputFormat.setOutputPath(job, dir);
+      context = new TaskAttemptContext(job.getConfiguration(),
+          new TaskAttemptID());
+      HFileOutputFormat hof = new HFileOutputFormat();
+      writer = hof.getRecordWriter(context);
+
+      // write out random rows
+      writeRandomKeyValues(writer, context, ROWSPERSPLIT);
+      writer.close(context);
+
+      // Make sure that a directory was created for every CF
+      FileSystem fileSystem = dir.getFileSystem(conf);
+      
+      // commit so that the filesystem has one directory per column family
+      hof.getOutputCommitter(context).commitTask(context);
+      for (byte[] family : FAMILIES) {
+        String familyStr = new String(family);
+        boolean found = false;
+        for (FileStatus f : fileSystem.listStatus(dir)) {
+
+          if (Bytes.toString(family).equals(f.getPath().getName())) {
+            // we found a matching directory
+            found = true;
+
+            // verify that the compression on this file matches the configured
+            // compression
+            Path dataFilePath = fileSystem.listStatus(f.getPath())[0].getPath();
+            Reader reader = new HFile.Reader(fileSystem, dataFilePath, null, false);
+            reader.loadFileInfo();
+            assertEquals("Incorrect compression used for column family " + familyStr
+                         + "(reader: " + reader + ")",
+                         configuredCompression.get(familyStr), reader.getCompressionAlgorithm());
+            break;
+          }
+        }
+
+        if (!found) {
+          fail("HFile for column family " + familyStr + " not found");
+        }
+      }
+
+    } finally {
+      dir.getFileSystem(conf).delete(dir, true);
+    }
+  }
+
+
+  /**
+   * @return
+   */
+  private Compression.Algorithm[] getSupportedCompressionAlgorithms() {
+    String[] allAlgos = HFile.getSupportedCompressionAlgorithms();
+    List<Compression.Algorithm> supportedAlgos = Lists.newArrayList();
+
+    for (String algoName : allAlgos) {
+      try {
+        Compression.Algorithm algo = Compression.getCompressionAlgorithmByName(algoName);
+        algo.getCompressor();
+        supportedAlgos.add(algo);
+      }catch (Exception e) {
+        // this algo is not available
+      }
+    }
+
+    return supportedAlgos.toArray(new Compression.Algorithm[0]);
+  }
+
+
+  /**
+   * Write random values to the writer assuming a table created using
+   * {@link #FAMILIES} as column family descriptors
+   */
+  private void writeRandomKeyValues(RecordWriter<ImmutableBytesWritable, KeyValue> writer, TaskAttemptContext context,
+      int numRows)
+      throws IOException, InterruptedException {
+    byte keyBytes[] = new byte[Bytes.SIZEOF_INT];
+    int valLength = 10;
+    byte valBytes[] = new byte[valLength];
+
+    int taskId = context.getTaskAttemptID().getTaskID().getId();
+    assert taskId < Byte.MAX_VALUE : "Unit tests dont support > 127 tasks!";
+
+    Random random = new Random();
+    for (int i = 0; i < numRows; i++) {
+
+      Bytes.putInt(keyBytes, 0, i);
+      random.nextBytes(valBytes);
+      ImmutableBytesWritable key = new ImmutableBytesWritable(keyBytes);
+
+      for (byte[] family : TestHFileOutputFormat.FAMILIES) {
+        KeyValue kv = new KeyValue(keyBytes, family,
+            PerformanceEvaluation.QUALIFIER_NAME, valBytes);
+        writer.write(key, kv);
+      }
+    }
+  }
+  
   public static void main(String args[]) throws Exception {
     new TestHFileOutputFormat().manualTest(args);
   }
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java b/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
index 110cd3f..3170ab3 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
@@ -54,5 +54,6 @@ public class TestCompressionTest {
     assertFalse(CompressionTest.testCompression("LZO"));
     assertTrue(CompressionTest.testCompression("NONE"));
     assertTrue(CompressionTest.testCompression("GZ"));
+    assertTrue(CompressionTest.testCompression("SNAPPY"));
   }
 }
-- 
1.7.0.4

